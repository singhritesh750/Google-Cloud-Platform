click to Activate google cloud shell on top right corner
click to "Continue"
Then apears the Shell Terminal with "Welcome to Cloud Shell!" message.

There is 2 information given on this page:
(a) the project id i.e., IT AUTOMATICALLY CONNECTS TO YOUR CURRENT PROJECT
(b) the "gcloud config set project [PROJECT_ID]" to change to a different project

GITHUB CMD COMMANDS:
1.	echo "# Google-Cloud-Platform" >> README.md
2.	git init
3.	git add README.md
4.	git commit -m "first commit"
5.	git branch -M main
6.	git remote add origin https://github.com/singhritesh750/Google-Cloud-Platform.git
7.	git push -u origin main

Back to GCS terminal->
"gcloud help" to get hep
"gcloud version" to get version
Afterall its a normal linux machine so all the commands working there will also work here as well
"pwd" -> check the present directory 
ls -l -> to check the directory
cat R[tab button] -> to read the file inside the directory
 
** [first name of the file ][tab button] -> Auto completes the name of the file that FIRST APPEARS WITH THE ALPHABET
 
* Few informations to notice in the above command return:
 (a) VM is ephemeral -> it is temporary so anything that you want to be saved, should be saved
 (b) home directory will persist accross sessions -> so everything should be saved here
 (c) "dl [file_name]" -> to download the file
 
 
NOW, steps to starting working with the git file:
"git clone [full_https_link]" -> to get the lab files from the git
"ls -l" -> SEE IT CREATES A NEW DIRECTORY FOR US
"cd gcp-cloud-engineer["/" -> optional]" -> Let's navigate into that directory.
"cd Cloud-shell-Hello["/" -> optional]" -> Now inside there there's another folder.


* This directory has a couple of source files for no JSA applications.
  "node Hello.js" -> runnig it tells us that it's running an application on its local hosts address on Port 8080

** BUT if we try navigating to that address from browser then we'll see that it doesn't work because the local host for this browser is different than one for CS(Web Preview)

"Web preview" -> URL to connect us through to the application


Now, Important point to focus:
we get an "appspot.com" URL to connect us through to the application that we're running in CS.
IT'S Registrant Organization: Google LLC
AppSpot is an app launcher.


"node [file_name.js]" -> to run the file
Can edit the content on the .js file either
 (1) in shell or
 (2) on the cloud editor.
"ctrl+c" -> to stop the server running for the port 8080 by "node [file_name.js]"
"nodemon [file_name.js]" -> to run and load the changes in the source file into the node. it's a tool that comes pre-installed in the CS.


Now, to update the Lab-file from the terminal using commands
(1). git clone https://github.com/ACloudGuru/gcp-cloud-engineer.git
(2). ls -l
(3). cd gcp-cloud-engineer/
(4). ls -l
(5). (a) If update.sh is executable i.e., green in color, then -> cd update.sh
     (b) if not then first make it executable -> chmod +x upudate.sh
                                              -> cd update.sh
Once above steps are done then from now on, just run the update script -> cd update.sh


Using GC Storage:
(1). first make the Bucket from the console with globally unique name
(2). thne open the bucket and upload some files into it
Clicking the files open them. URL of these files are long because they are signed and BY DEFAULT PRIVATELY ACCESSIBLE
to make it publicly accessible : (1). far right corner click 3 dots
                                  (2). open permissions then add user name it alluser and make it reader
                                  

Bucket:
(1). GC storage store all sorts of objects but inside of a bucket (container).
(2). Name for bucket MUST BE unique across cloud storage i.e., just like Project I.D. bucket names are globally unique.
(3). The location of a bucket cannot be changed after the bucket has been created.
(4). The labels of bucket can be changed.
(5). the default storage class can be changed for new objects that are put in this bucket.
(6). Permission granted to the bucket can be changed even after creating it.

NOTE: (1). The URL request for every file inside the bucket, to open, is very long because it's a signed request to be able to view this file.
           Every file is private. Not just anyone is allowed to view this file by default.
      (2). "URL can end with either object or can be folder".
           It does not have a hierarchical file system. If neede, Hierarchy can be created by ending object name with "/". And then it'll act as folders.
           Every object whose name ends with a "/", is considered to be a folder and you can navigate into that folder and upload files there if you want.

gsutil mv -p gs:// storage-lab-console/test/[file_name] git -> gsutil_equivalent for rename files in bucket

gsutil:
(1). Using "gsutil" (Previously using the console web app)
(2). It's a command-line tool to connect to GC storage and it mimics many of the commands in a normal shell prompt. e.g., if we type "gsutil ls" it shows us the buckets.

gcloud config list -> To verify that we're in the right project & account

NOTE: the native GC storage addresses (bucket name) starts with "gs://" similar to how Web addresses start with "HTTPS://"

gsutil ls gs://[bucket_name]/ -> to show the content of bucket ( NOTE: This shows folder but not it's content)
gsutil ls gs://[bucket_name]/** -> to list everything (shows folder + it's content)
gs mb --help -> this can give you a few pages of good information on how to make a bucket
gsutil mb -l [bucket_region]["using gs://" new_bucket_name] -> creates the new bucket

NOTE: if "-l" is not mentioned then it will create the bucket in the us region by default.

gsutil label get [gs://bucket_name]/ ->  reult is json document listing all the labels of this bucket.
gsutil label get [gs://bucket_name]/ > [new_filename.json] -> send the json result of bucket_name to new_filename.json

">" symbol - run the thing before it and put the output into the file with the name that comes after it.

cat new_filename.json -> result is same json labels
gsutil label get [gs://new_bucket_name]/ -> result is no label
gsutil label set [new_filename.json] [gs://new_bucket_name] -> to set label to new_bucket_name

To add more labels to a particular bucket (2 ways):
(1). get a copy of the current labels, Edit The .json File locally, then set those labels back. (harder way)
(2). gsutil label ch -l "extra-label":"extra value" bucket_name -> let us change the labels to add an extra-label and values. (smarter way)

*There are some things that can only be done through the command line like object versioning control. It can be done only programmatically whether that's through the
(1). command line
(2). an API call

gsutil versioning get gs://[bucket name]/ -> to see whether versioning is turned on for a particular bucket.
gsutil versioning set on gs://[bucket name]/ -> to change the versioning to "ON".


gsutil ls -a gs://[bucket_name]/ -> to include information about archived objects. Basically, it tells to show information about object versioning.

NOTE: file that we uploaded includes a generation number.

**NOTE: if we do something like remove this file, then running the normal list command will show us that it's gone but running the list -a command will show us that it's 
still there. That's basically what object versioning does. It means that every time we give GC an object, it will hold on to it forever until we tell it quite specifically 
that we want that object to be deleted. Giving it a new version of that object or just deleting it by name isn't enough.

NOTE:
(1). gsutil ls gs://[bucket_name]/ -> don't show the archived files. Similarly, in the console again we didn't see archived versions (by default).
(2). when we copy objects from the other bucket we didn't copy their sharing settings. they were shared publicly in the console bucket but in new_bucket they're private 
because that's how the command line bucket is setup.

gsutil acl ch -u AllUsers:R gs://[object_name_path] -> to share an object (privately accessed)
acl -> for access control list
ch -> for change
-u -> for user say all users : R for read access
object_name_path -> the file_name that we want
the object will now be available in the same way as the other publicly accessed object.

Start First GCE VM Launch in Cloud Compute Engine (Cloud Compute Engine Setup):
First To check the right project is selected or not :
(1). gcloud config get-value project -> and it shows us that we are indeed in the services exploration labs project.
(2). can see the same thing up here in this cloud_shell tab label.
(3). gcloud config list

gcloud compute instances list -> to check that there aren't running VM in Google compute engine yet

NOTE: If the result of this API call was "HTTPError 403" that means that we're not allowed to do that.

*** The reason to go to the service accounts list before navigating to the compute engine section of the console is that to show that some service accounts are created 
automatically after starting the Compute engine API.***

gcloud services list -> this shows us many different services (by default shows the enabled ones).
gcloud services list -h -> to get SMALL help about list 
gcloud services list --enabled -> This list is the API is that are enabled for this specific project.

NOTE: these APIs that we're looking at right now are enabled by default for all new projects & this is the same reason that we didn't have any trouble when we did the 
GC storage.

gcloud services list --available -> to show the available services
gcloud services list --available | grep compute -> to pipe the output of that command indicated by this "|" to the command grep compute.
pipe syntax "|" - takes the output of command on the left & passes it as input to one on the right
grep - grep will only show lines that include the word compute.
Here compute.googleapis.com is available. So to enable service :

gcloud services -h -> to see what things we're allowed to do at this level (disable/enable/list)
(1). gcloud services enable compute googleapis.com -> to enable the service
(2). gcloud compute instances list -> say no to the prompt it gives us a URL that we can follow and enable the API.
(3). go to the COMPUTE ENGINE section of the console - it enable the compute engine API automatically.
(4). API & services section in the console - easy links to "Enable APIS & services".
